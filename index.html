<!DOCTYPE html>

<head>
    <title>MLG 2025</title>
    <link rel="stylesheet" type="text/css" href="neat.css">
    <link rel="icon" type="image/x-icon" href="favicon.png">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta charset="UTF-8">
</head>

<body>

<ol class="threecol">
  <li><a href="#dates">Dates</a></li>
  <li><a href="#keynotes">Keynote Speakers</a></li>
  <li><a href="#call">Call for Papers</a></li>
  <li><a href="#schedule">Schedule</a></li>
  <li><a href="#accepted">Accepted Papers</a></li>
  <li><a href="#awards">Awards</a></li>
  <li><a href="#contact">Organizers and PC</a></li>
  <li><a href="#history">Previous Workshops</a></li>
</ol>


<h1 id="about">22nd International Workshop on<br>
Mining and Learning with Graphs</h1>

<center><em> Monday, 15th September 2025, Porto, jointly with <a href="https://ecmlpkdd.org/2025/" target=_blank>ECMLPKDD2025</a></em></center> <br />

<img src="oporto.webp" class="headerimg" width="70%" height="auto" alt="GPT-4o generated picture of the bridge in porto with a graph floating behind it.">

<!--

<h2 id="news">News</h2>
<ol>
  <li>We will organize <a href="https://mlg-europe.github.io">MLG@ECMLPKDD 2024</a>, the 22th Workshop on Mining and Learning with Graphs. Join us for interesting discussions about graphs in Vilnius, Lithuania!</li>
</ol>

-->

<h2 id="dates">Important Dates</h2>

<ul class="centered">
  <li><strong>Paper <a href="https://cmt3.research.microsoft.com/ECMLPKDDWorkshopTrack2025">submission</a> deadline</strong>: <s>14.06.2025</s>  19.06.2025</li>
  <li><strong>Paper acceptance notification</strong>: 14.07.2025</li>
  <li><strong>Camera ready submission deadline</strong>: 15.08.2025</li>
  <li><strong>Workshop date:</strong> Monday 15. September 2025</li>
  <li>All deadlines expire 23:59 AoE</li>
</ul>

<h2 id="keynotes">Keynote Speakers</h2>

<ul class="keynotes">
<li class="center">
  <img src="https://rusty1s.github.io/dist/46c1d914fb6cc4d67d96c26b2fbd2573.png" width="150px" height="auto"><br/>
  <a href="https://rusty1s.github.io/#/" target=_blank>Matthias Fey</a><br />
  <a href="https://kumo.ai/" target=_blank>kumo.ai</a>
</li>
<li class="center">
  <!-- <h3>Embedding and Clustering of Attributed Multiplex Networks</h3> -->
  <p>Matthias is the creator of PyTorch Geometric and a founding engineer at kumo.ai. He obtained his PhD in Machine Learning on graphs from the TU Dortmund University. His main area of interest lies in the generalization of Deep Learning methods to a wide range of applications related to structured data.
  </p> 
</li>
<li class="center">
  <img src="rebekka.png" width="150px" height="auto"><br/>
  <a href="https://sites.google.com/view/rebekkaburkholz/" target=_blank>Rebekka Burkholz</a><br />
  Helmholtz Center CISPA
</li>
<li class="center">
  <!-- <h3>Exploiting Symmetries for Learning in Deep Weight Spaces</h3> -->
  <p>Rebekka is a faculty member at the CISPA Helmholtz Center for Information Security in Saarbrücken, where she leads the relational machine learning group. Her main goal to develop efficient deep learning algorithms that are robust to noise, require small sample sizes, and are generally applicable in the sciences. Her work is founded in theory with implications for real world applications and is often characterized by a complex network science perspective. Her favourite applications and sources of inspiration are currently the biomedical domain, pharmacy, and physics. Her group is supported by the ERC starting grant SPARSE-ML.
  </p>
</ul>


<h2 id="call">Call for Papers</h2>

<p>This workshop is a forum for exchanging ideas and methods for mining and learning with graphs, developing new common understandings of the problems at hand, sharing data sets where applicable, and leveraging existing knowledge from different disciplines. The goal is to bring together researchers from academia and industry to create a forum to discuss recent advances in graph analysis. 
In doing so, our aim is to better understand the overarching principles and limitations of current methods and to inspire research on new algorithms and techniques for mining and learning with graphs.</p>

<p>To reflect the broad scope of work on mining and learning with graphs, we encourage submissions that span the spectrum from theoretical analysis to algorithms and implementation to applications and empirical studies. 
We are interested in the full spectrum of graph data, including but not limited to attributed graphs, labeled graphs, knowledge graphs, evolving graphs, transactional graph databases, etc.</p>

<p><b>We therefore invite submissions on <i>theoretical aspects, algorithms and methods</i>, and <i>applications</i> of the following (non-exhaustive) list of areas:</b></p>

<ul>    
    <li> Computational or statistical learning theory related to graphs
    </li>
    <li> Theoretical analysis of graph algorithms or models
    </li>
    <li> Semi-supervised learning, online learning, active learning, transductive inference, and transfer learning in the context of graphs
    </li>
    <li> Unsupervised learning and graph clustering
    </li>
    <li> Interesting pattern mining on graphs and community detection
    </li>
    <li> Graph kernels and metric learning on graphs
    </li>
    <li> Graph and vertex embeddings and representation learning on graphs
    </li>
    <li> Solving combinatorial problems on graphs with ML / data-driven combinatorial optimization
    </li>
    <li> Explainable, fair, robust, and/or privacy-preserving ML on graphs
    </li>
    <li> Statistical models of graphs and graph sampling

    </li>
    <li> Analysis of social media, chemical or biological networks, infrastructure networks, knowledge graphs
    </li>
    <li> Benchmarking and reproducibility aspects of graph-based learning
    </li>
    <li> Libraries and tools for all of the above areas
    </li>
</ul>

<p><b>We welcome many kinds of papers, such as, but not limited to:</b></p>

<ul>
    <li> Novel research papers
    </li>
    <li> Demo papers
    </li>
    <li> Dataset papers
    </li>
    <li> Work-in-progress papers
    </li>
    <li> Visionary papers (white papers)
    </li>
    <li> Appraisal papers of existing methods and tools (e.g., lessons learned)
    </li>
    <li> Relevant work that has been previously published
    </li>
    <li> Work that will be presented at the main conference (can be submitted with the regular 16-page limit of ECMLPKDD)
    </li>
</ul>

<p><b>Submission Guidelines</b>: Authors should clearly indicate in their abstracts the kinds of submissions that the papers belong to, to help reviewers better understand their contributions.
All papers will be peer-reviewed (single-blind). 
Submissions must be in PDF, long papers no more than 12 pages long, short papers no more than 8 pages long, formatted according to the standard Springer LNCS style required for ECMLPKDD submissions.
References and appendix do not count towards the page limit.
The accepted papers will be published on the workshop website and will not be considered archival for resubmission purposes.
Authors whose papers are accepted to the workshop will have the opportunity to participate in a pitch and poster session, and the best two will also be chosen for oral presentation.</p>

<p><b>Papers should be submitted via CMT</b>: <a href="https://cmt3.research.microsoft.com/ECMLPKDDWorkshopTrack2025">https://cmt3.research.microsoft.com/ECMLPKDDWorkshopTrack2025</a>. Please select the <b>MLG: Mining and Learning with Graphs</b> track.
<!-- Please select the track <i>MLG: 22nd Workshop on Mining and Learning with Graphs</i>. --></p>

<p><b>Post-Workshop Springer Proceedings</b>: High quality, original, non-dual-submitted papers will be invited to be published in post-workshop proceedings, assuming that ECMLPKDD offers them as in previous years.


<p><b>Dual Submission Policy</b>:
We accept submissions that are currently under review at other venues. 
However, in this case, our page limits apply.
Please also check the dual submission policy of the other venue.</p>



<h2 id="schedule">Tentative Schedule</h2>


<table class="centered">



    <tr><td valign="top">9.00h</td> <td> <b>Welcoming</td> </tr>
    <tr><td valign="top">9.15h</td> <td> <b>Keynote</b><br />
    Rebekka Burkholz
    </td> </tr>
    <tr><td valign="top">10.15h</td> <td> <b>Spotlight Talks (Group A)</b>
    </td> </tr>
    <tr><td valign="top">10.30h</td> <td> <b>Coffee + Poster Session (Group A) </b>
    </td> </tr>
    <tr><td valign="top">12.00h</td> <td> <b>Contributed Talk</b><br />
    Katharina Limbeck, Lydia Mezrag, Guy Wolf, Bastian Rieck:<br />
    Geometry-Aware Edge Pooling for Graph Neural Networks.
    </td> </tr>
    <tr><td valign="top">12.15h</td> <td> <b>Contributed Talk</b><br />
    Patrick Indri, Tamara Drucks, Thomas Gärtner:<br />
    Private and Expressive Graph Representations.
    </td> </tr>
    <tr><td valign="top">12.30h</td> <td> <b>Lunch Break</b></td> </tr>
    <tr><td valign="top">14.00h</td> <td> <b>Keynote</b><br />Matthias Fey</td></tr>
    <tr><td valign="top">15.00h</td> <td> <b>Contributed Talk</b><br />
    Christoph Sandrock, Sebastian Lüderssen, Maximilian Thiessen, Thomas Gärtner:<br />
    Efficient Minimization of Peakless Functions on Bounded-degree Graphs.
    </td> </tr>
    <tr><td valign="top">15.15h</td> <td> <b>Contributed Talk</b><br />
    Dionisia Naddeo, Tiago Azevedo, Nicola Toschi:<br />
    Do We Need Curved Spaces? A Critical Look at Hyperbolic Graph Learning in Graph Classification.
    </td> </tr>
    <tr><td valign="top">15.30h</td> <td> <b>Spotlight Talks (Group B)</b></td> </tr>
    <tr><td valign="top">15.45h</td> <td> <b>Coffee + Poster Session (Group B)</b> </td> </tr>
     <tr><td valign="top">17.15h</td> <td> <b>Contributed Talk</b><br />
     Pavel Prochazka, Michal Mares, Lukas Bajer:<br />
     Contrastive Learning as Optimal Homophilic Graph Structure Learning.
    </td> </tr>
    <tr><td valign="top">17.30h</td> <td> <b>Contributed Talk</b><br />
     Adrian Arnaiz-Rodriguez, Federico Errica : <br />
     Oversmoothing, "Oversquashing", Heterophily, Long-Range, and more: Demystifying Common Beliefs in Graph Machine Learning.
    </td> </tr>
    <tr><td valign="top">17.45h</td> <td> <b>Closing remarks and Awards</b></td> </tr>

    <!--<tr><th>Timeslot</th><th>Activity</th>
    <tr><td>9.00-9.15</td> <td> Welcoming </td> </tr>
    <tr><td>9.15-10.15</td> <td> Keynote 1 </td> </tr>
    <tr><td>10.15-10.30</td> <td> Spotlights 1 </td> </tr>
    <tr><td>10.30-12.00</td> <td> 1st poster session w/ coffee break </td> </tr>
    <tr><td>12.00-12.15</td> <td> 1st oral talk </td> </tr>
    <tr><td>12.15-12.30</td> <td> 2nd oral talk </td> </tr>
    <tr><td>12.30-14.00</td> <td> Lunch break </td> </tr>
    <tr><td>14.00-15.00</td><td> Keynote 2</td></tr>
    <tr><td>15.00-15.15</td><td> 3rd oral talk</td></tr>
    <tr><td>15.15-15.30</td><td> 4th oral talk</td></tr>
    <tr><td>15.30-15.45</td><td> Spotlights 2 </td></tr>
    <tr><td>15.45-17.15</td><td> 2nd poster session w/ coffee break </td></tr>
    <tr><td>17.15-17.30</td> <td> 5ft oral talk </td> </tr>
    <tr><td>17.30-17.45</td> <td> 6th oral talk </td> </tr>
    <tr><td>17.45-18.00</td><td>Closing remarks & Awards</td></tr> -->


</table>



 
 <h2 id="accepted">Accepted Papers</h2> 
 <!-- this will be replaced by autogenerated list -->
 <ol>
<li>
Adarsh Jamadandi, Celia Rubio-Madrigal, Rebekka Burkholz (2025):<br />
<a title="Message Passing Graph Neural Networks are known to suffer from two problems that are sometimes believed to be diametrically opposed: over-squashing and over-smoothing. The former results from topological bottlenecks that hamper the information flow from distant nodes and are mitigated by spectral gap maximization, primarily, by means of edge additions. However, such additions often promote over-smoothing that renders nodes of different classes less distinguishable. Inspired by the Braess phenomenon, we argue that deleting edges can address over-squashing and over-smoothing simultaneously. This insight explains how edge deletions can improve generalization, thus connecting spectral gap optimization to a seemingly disconnected objective of reducing computational resources by pruning graphs for lottery tickets. To this end, we propose a computationally effective spectral gap optimization framework to add or delete edges and demonstrate its effectiveness on the long range graph benchmark and on larger heterophilous datasets." href="papers/submission/188/Submission/Spectral_Pruning_MLG25_2.pdf">Spectral Graph Pruning Against Over-Squashing and Over-Smoothing</a>.<br />
<p class="discreet">
[group A]
[<a href="papers/submission/188/Submission/Spectral_Pruning_MLG25_2.pdf">pdf</a>]
</p>
</li>
<li>
Adrian Arnaiz-Rodriguez, Federico Errica (2025):<br />
<a title="After a renaissance phase in which researchers revisited the message-passing paradigm through the lens of deep learning, the graph machine learning community shifted its attention towards a deeper and practical understanding of message-passing's benefits and limitations. In this position paper, we notice how the fast pace of progress around the topics of oversmoothing and oversquashing, the homophily-heterophily dichotomy, and long-range tasks, came with the consolidation of commonly accepted beliefs and assumptions that are not always true nor easy to distinguish from each other. We argue that this has led to ambiguities around the investigated problems, preventing researchers from focusing on and addressing precise research questions while causing a good amount of misunderstandings. Our contribution wants to make such common beliefs explicit and encourage critical thinking around these topics, supported by simple but noteworthy counterexamples. The hope is to clarify the distinction between the different issues and promote separate but intertwined research directions to address them." href="">Oversmoothing, "Oversquashing", Heterophily, Long-Range, and more: Demystifying Common Beliefs in Graph Machine Learning</a>.<br />
<p class="discreet">
[group B]
</p>
</li>
<li>
Alessio Comparini, Lea Schmidt, Vanessa Siffredi, Damien Marie, Clara James, Jonas Richiardi (2025):<br />
<a title="The integration of structural and functional brain connectivity provides a holistic view of the brain's organization, but its application in Graph Neural Network (GNN) models for predicting "brain age" is understudied, and a systematic benchmark of optimal data fusion strategies is currently lacking. We systematically benchmark the performances of early and late fusion  multimodal architectures  against single-modality models for brain age preddiction using structural and functional connectcomes, using five different GNN backbones on 747 healty partecipants  (median age, 16.3 years and IQR 13.5-18.5 years) obtained from the Philadelphia Neurodevelopmental Cohort. The late fusion architecture improved performance over the structural-only baseline in three of five models, with the GCN model achieving the highest overall score in cross-validation ( R^2=0.639 +- 0.05). The early fusion architecture showed inconsistent results and did not offer a reliable improvement over the single-modality baseline. Finally is observed that optimal model architecture depends on the data type: structural brain graphs favors deep, narrow models to capture their hierarchy, whereas functional brain graphs requires wider, shallower models." href="papers/cameraReady/336/CameraReady/MLG20251.pdf">Late and Early Fusion Graph Neural Network Architectures for Integrative Modeling of Multimodal Brain Connectivity Graphs</a>.<br />
<p class="discreet">
[group B]
[<a href="papers/cameraReady/336/CameraReady/MLG20251.pdf">pdf</a>]
</p>
</li>
<li>
Andreas Roth, Thomas Liebig (2025):<br />
<a title="This work is accepted for publication at IJCAI 2025.  Most graph neural networks (GNNs) utilize approximations of the general graph convolution derived in the graph Fourier domain. While GNNs are typically applied in the multi-input multi-output (MIMO) case, the approximations are performed in the single-input single-output (SISO) case. In this work, we first derive the MIMO graph convolution through the convolution theorem and approximate it directly in the MIMO case. We find the key MIMO-specific property of the graph convolution to be operating on multiple computational graphs, or equivalently, applying distinct feature transformations for each pair of nodes. As a localized approximation, we introduce localized MIMO graph convolutions (LMGCs), which generalize many linear message-passing neural networks. For almost every choice of edge weights, we prove that LMGCs with a single computational graph are injective on multisets, and the resulting representations are linearly independent when more than one computational graph is used. Our experimental results confirm that an LMGC can combine the benefits of various methods." href="papers/submission/88/Submission/MIMO_GCs_MLG.pdf">What Can We Learn From MIMO Graph Convolutions?</a>.<br />
<p class="discreet">
[group A]
[<a href="papers/submission/88/Submission/MIMO_GCs_MLG.pdf">pdf</a>]
</p>
</li>
<li>
Celia Rubio-Madrigal, Adarsh Jamadandi, Rebekka Burkholz (2025):<br />
<a title="Maximizing the spectral gap through graph rewiring has been proposed to enhance the performance of message-passing graph neural networks (GNNs) by addressing over-squashing. However, as we show, minimizing the spectral gap can also improve generalization. To explain this, we analyze how rewiring can benefit GNNs within the context of stochastic block models. Since spectral gap optimization primarily influences community strength, it improves performance when the community structure aligns with node labels. Building on this insight, we propose three distinct rewiring strategies that explicitly target community structure, node labels, and their alignment: (a) community structure-based rewiring (ComMa), a more computationally efficient alternative to spectral gap optimization that achieves similar goals and (b) feature similarity-based rewiring (FeaSt), which focuses on maximizing global homophily and and (c) a hybrid approach (ComFy), which enhances local feature similarity while preserving community structure to optimize label-community alignment. Extensive experiments confirm the effectiveness of these strategies and support our theoretical insights." href="papers/submission/198/Submission/ComFy_MLG25.pdf">GNNs Getting ComFy: Community and Feature Similarity Guided Rewiring</a>.<br />
<p class="discreet">
[group B]
[<a href="papers/submission/198/Submission/ComFy_MLG25.pdf">pdf</a>]
</p>
</li>
<li>
Christoph Sandrock, Sebastian Lüderssen, Maximilian Thiessen, Thomas Gärtner (2025):<br />
<a title="We study the problem of query-efficiently minimizing peakless functions on graphs. Peakless functions are a notion of convex functions on the nodes of a graph and are defined by having no local ``peaks'' along any shortest paths. We demonstrate that peakless functions form a rich class and are non-trivial to optimize on chordal graphs. Further, we show that these functions can be minimized using at most $4 Δ łog n$ queries on graphs with $n$ nodes and maximum degree $Δ$, where each query reveals the function value of a selected node. We complement this result by providing a nearly matching lower bound on the query complexity of $Ømega(\fracΔłog Δłog n)$." href="papers/cameraReady/277/CameraReady/efficient_minimitazion_of_peakless_functions_on_graphs.pdf">Efficient Minimization of Peakless Functions on Bounded-degree Graphs</a>.<br />
<p class="discreet">
[group B]
[<a href="papers/cameraReady/277/CameraReady/efficient_minimitazion_of_peakless_functions_on_graphs.pdf">pdf</a>]
</p>
</li>
<li>
Dehn Xu, Tim Katzke, Emmanuel Müller (2025):<br />
<a title="Graph Neural Networks (GNNs) have emerged as a powerful approach for graph-based machine learning tasks. Previous work applied GNNs to image-derived graph representations for various downstream tasks such as classification or anomaly detection. These transformations include segmenting images, extracting features from segments, mapping them to nodes, and connecting them. However, to the best of our knowledge, no study has rigorously compared the effectiveness of the numerous potential image-to-graph transformation approaches for GNN-based graph-level anomaly detection (GLAD). In this study, we systematically evaluate the efficacy of multiple segmentation schemes, edge construction strategies, and node feature sets based on color, texture, and shape descriptors to produce suitable image-derived graph representations to perform graph-level anomaly detection. We conduct extensive experiments on dermoscopic images using state-of-the-art GLAD models, examining performance and efficiency in purely unsupervised, weakly supervised, and fully supervised regimes. Our findings reveal, for example, that color descriptors contribute the best standalone performance while incorporating shape and texture features consistently enhances detection efficacy. In particular, our best unsupervised configuration using OCGTL achieves a competitive AUC-ROC score of up to 0.805 without relying on pretrained backbones like comparable image-based approaches. With the inclusion of sparse labels, the performance increases substantially to 0.872 and with full supervision to 0.914 AUC-ROC." href="papers/cameraReady/280/CameraReady/Graph_AD_Images_CR.pdf">From Pixels to Graphs: Deep Graph-Level Anomaly Detection on Dermoscopic Images</a>.<br />
<p class="discreet">
[group A]
[<a href="papers/cameraReady/280/CameraReady/Graph_AD_Images_CR.pdf">pdf</a>]
</p>
</li>
<li>
Dionisia Naddeo, Tiago Azevedo, Nicola Toschi (2025):<br />
<a title="Hyperbolic geometry has gained attention for its ability to naturally embed hierarchical and tree-like structures with low distortion, outperforming Euclidean spaces in various graph representation tasks. While previous work has demonstrated the advantage of graph neural networks embedding in hyperbolic space for link prediction and node classification, the benefits for graph classification, and especially the role of node features, remain less understood. These studies typically attribute the benefits of hyperbolic models to the hierarchical or tree-like nature of graph structures, often neglecting the important role that node features play in leveraging these geometric advantages. With this in mind, we design an experiment specifically aimed at evaluating the interplay between geometry and node features in graph classification, creating a dataset composed exclusively of tree-structured graphs. Each graph is generated by sampling the number of children per node at each level from a predefined range of branching factors, which varies across levels. The dataset defines two distinct classes based on these branching factor patterns. Nodes are characterized by either random features or structural embeddings obtained via node2vec using breadth-first search (BFS)-biased random walks, designed to emphasize local neighborhood structure and capture hierarchical structures within the graph. We evaluated and compared graph neural networks with node embeddings learned in different geometries— hyperbolic vs. Euclidean— in low-dimensional latent spaces. Results show that hyperbolic geometry outperforms Euclidean models, with the largest gains observed when node features encode local hierarchical context. These findings underscore that the performance advantage of hyperbolic models does not stem solely from the tree-like structure of the graph and rather, effective feature design that captures the underlying hierarchy is essential to fully exploit the benefits of hyperbolic geometry." href="papers/cameraReady/274/CameraReady/HGCN_MLG2025_final.pdf">Do We Need Curved Spaces? A Critical Look at Hyperbolic Graph Learning in Graph Classification</a>.<br />
<p class="discreet">
[group B]
[<a href="papers/cameraReady/274/CameraReady/HGCN_MLG2025_final.pdf">pdf</a>]
</p>
</li>
<li>
Giorgio Venturin, Ilie Sarpe, Fabio Vandin (2025):<br />
<a title="[Paper accepted at the ECML-PKDD 2025 research track]  Triangle counting is a fundamental and widely studied problem on static graphs, and recently on temporal graphs, where edges carry information on the timings of the associated events. Streaming processing and resource efficiency are crucial requirements for counting triangles in modern massive temporal graphs, with millions of nodes and up to billions of temporal edges. However, current exact and approximate algorithms are unable to handle large-scale temporal graphs.  To fill such a gap, we introduce STEP, a scalable and efficient algorithm to approximate temporal triangle counts from a stream of temporal edges.  STEP combines predictions of the number of triangles a temporal edge is involved in, with a simple sampling strategy, leading to scalability, efficiency, and accurate approximation of all eight temporal triangle types simultaneously. We analytically prove that, by using a sublinear amount of memory, STEP obtains unbiased and very accurate estimates.  In fact, even noisy predictions can significantly reduce the variance of STEP's estimates.  Our extensive experiments on massive temporal graphs with up to billions of edges demonstrate that STEP outputs high-quality estimates and is more efficient than state-of-the-art methods." href="papers/submission/202/Submission/paper.pdf">Efficient Approximate Temporal Triangle Counting in Streaming with Predictions</a>.<br />
<p class="discreet">
[group B]
[<a href="papers/submission/202/Submission/paper.pdf">pdf</a>]
</p>
</li>
<li>
Jakub Peleška, Gustav Šír (2025):<br />
<a title="Relational Deep Learning (RDL) is an emerging paradigm that leverages Graph Neural Network principles to learn directly from relational databases by representing them as heterogeneous graphs. However, existing RDL models typically rely on task-specific supervised learning, requiring training separate models for each predictive task, which may hamper scalability and reuse.  In this work, we propose a novel task-agnostic contrastive pretraining approach for RDL that enables database-wide representation learning. For that aim, we introduce three levels of contrastive objectives—row-level, link-level, and context-level—designed to capture the structural and semantic heterogeneity inherent to relational data. We implement the respective pretraining approach through a modular RDL architecture and an efficient sampling strategy tailored to the heterogeneous database setting. Our preliminary results on standard RDL benchmarks demonstrate that fine-tuning the pretrained models measurably outperforms training from scratch, validating the promise of the proposed methodology in learning transferable representations for relational data." href="papers/cameraReady/335/CameraReady/Task-Agnostic_Contrastive_Pretraining_for_Relational_Deep_Learning_CR.pdf">Task-Agnostic Contrastive Pretraining for Relational Deep Learning</a>.<br />
<p class="discreet">
[group B]
[<a href="papers/cameraReady/335/CameraReady/Task-Agnostic_Contrastive_Pretraining_for_Relational_Deep_Learning_CR.pdf">pdf</a>]
</p>
</li>
<li>
Joël Mathys, Henrik Christiansen, Federico Errica, Francesco Alesiani (2025):<br />
<a title="Accurately modeling long-range dependencies in graph structured data is critical for many real-world applications. However, properly incorporating long-range interactions beyond the nodes' immediate neighborhood remains an open challenge for graph machine learning models. Existing benchmarks for evaluating long-range capabilities cannot guarantee that their tasks actually depend on long-range information or are artificial. Therefore, claiming long-range modeling improvements based on empirical performance on those datasets remains a fragile and weak form of evidence. We introduce the Long-Range Ising Model (LRIM) Graph Benchmark, a physics-grounded benchmark based on the well-studied Ising model whose ground truth theoretically depends on long-range dependencies. Our benchmark consists of multiple datasets that scale from 256 to 65k nodes per graph and provide controllable long-range dependencies through multiple tunable parameters, allowing precise control over the hardness and "long-rangedness" of tasks. We provide model-agnostic evidence showing that local information is insufficient, further validating the design choices of our benchmark. This is ongoing, new research to provide a framework towards principled and provable long-range capability evaluation for graph machine learning." href="papers/cameraReady/141/CameraReady/LRIM___MLG25-compressed.pdf">Long Range Ising Model: A Benchmark for Long Range Capabilities in Graph Learning</a>.<br />
<p class="discreet">
[group A]
[<a href="papers/cameraReady/141/CameraReady/LRIM___MLG25-compressed.pdf">pdf</a>]
</p>
</li>
<li>
Katharina Limbeck, Lydia Mezrag, Guy Wolf, Bastian Rieck (2025):<br />
<a title="Abstract. Graph Neural Networks (GNNs) have shown significant success for graph-based tasks. Pooling layers are crucial components of GNNs that enable faster training and potentially better generalisation by reducing the size of input graphs. However, existing pooling operations often optimise for the learning task at the expense of fundamental graph structures and interpretability. This leads to unreliable performance across varying dataset types, downstream tasks and pooling ratios. Addressing these concerns, we propose novel graph pooling layers for structure-aware pooling via edge collapses. Our methods leverage diffusion geometry and iteratively reduce a graph’s size while preserving both its metric structure and structural diversity. We guide pooling using magnitude, an isometry-invariant diversity measure, which permits us to control the fidelity of the pooling process. Further, we use the spread of a metric space as a faster and more stable alternative ensuring computational efficiency. Empirical results demonstrate that our methods (i) achieve superior performance compared to alternative pooling layers across a range of diverse graph classification tasks, (ii) preserve key spectral properties of the input graphs, and (iii) retain high accuracy across varying pooling ratios." href="papers/cameraReady/146/CameraReady/_MLG2025__MagEdgePool.pdf">Geometry-Aware Edge Pooling for Graph Neural Networks</a>.<br />
<p class="discreet">
[group A]
[<a href="papers/cameraReady/146/CameraReady/_MLG2025__MagEdgePool.pdf">pdf</a>]
</p>
</li>
<li>
Lisi Qarkaxhija, Anatol Wegner, Ingo Scholtes (2025):<br />
<a title="In this work, we explore the use of untrained message passing layers in graph neural networks for link prediction. The untrained message passing layers we consider are derived from widely used graph neural network architectures by removing trainable parameters and nonlinearities in their respective message passing layers. Experimentally, we find that untrained message passing layers can lead to competitive and even superior link prediction performance compared to fully trained message passing layers while being more efficient and naturally interpretable, especially in the presence of high-dimensional features. We also provide a theoretical analysis of untrained message passing layers in the context of link prediction and show that the inner product of features produced by untrained message passing layers relate to common neighbour and path-based topological measures which are widely used for link prediction. As such, untrained message passing layers offer a more efficient and interpretable alternative to trained message passing layers in link prediction tasks." href="papers/submission/295/Submission/2025_ECML_UTMPNN.pdf">Link Prediction with Untrained Message Passing Layers</a>.<br />
<p class="discreet">
[group A]
[<a href="papers/submission/295/Submission/2025_ECML_UTMPNN.pdf">pdf</a>]
</p>
</li>
<li>
Manuel Dileo, Matteo Zignani, Sabrina Gaito (2025):<br />
<a title="Discrete-time temporal Graph Neural Networks (GNNs) are powerful tools for modeling evolving graph-structured data and are widely used in decision-making processes across domains such as social network analysis, financial systems, and collaboration networks. Explaining the predictions of these models is an important research area due to the critical role their decisions play in building trust in social or financial systems. However, the explainability of Temporal Graph Neural Networks remains a challenging and relatively unexplored field. Hence, in this work, we propose a novel framework to evaluate explainability techniques tailored for discrete-time temporal GNNs. Our framework introduces new training and evaluation settings that capture the evolving nature of temporal data, defines metrics to assess the temporal aspects of explanations, and establishes baselines and models specific to discrete-time temporal networks. Through extensive experiments, we outline the best explainability techniques for discrete-time GNNs in terms of fidelity, efficiency, and human-readability trade-offs. By addressing the unique challenges of temporal graph data, our framework sets the stage for future advancements in explaining discrete-time GNNs." href="papers/submission/126/Submission/4523_Evaluating_explainability-8.pdf">Evaluating explainability techniques on discrete-time graph neural networks</a>.<br />
<p class="discreet">
[group B]
[<a href="papers/submission/126/Submission/4523_Evaluating_explainability-8.pdf">pdf</a>]
</p>
</li>
<li>
Maximilian Seeliger, Fabian Jogl, Thomas Gärtner (2025):<br />
<a title="We propose a novel family of algorithms for generating expressive graph and node representations utilizing graph products. These representations, based on simple substructure counts in product graphs, aim to encode structural information that is often missed by standard message passing graph neural networks (MPNNs).  MPNNs allow to learn vector representations of graphs, that are used in critical domains like drug discovery, social network analysis, protein folding and transportation networks. Their expressiveness is limited by the Weisfeiler-Leman (WL) graph isomorphism test, meaning they are unable to distinguish certain non-isomorphic graphs and fail to recognize important substructures, which restricts their overall capability. Our approach, called  Product Substructure Count (PSC), addresses this by utilizing graph products to transform graphs. The transformed graphs encode extensive structural information within simple substructures, such as cycles. By counting these substructures at both the graph and node levels, we can generate embeddings that represent the graph as a whole as well as individual nodes.    We show that PSC representations outperform WL in isomorphism testing and improve the representational capacity of MPNNs across multiple benchmark datasets." href="papers/cameraReady/257/CameraReady/MLG2025-13.pdf">Graph Product Representations</a>.<br />
<p class="discreet">
[group A]
[<a href="papers/cameraReady/257/CameraReady/MLG2025-13.pdf">pdf</a>]
</p>
</li>
<li>
Namrata Banerji, Tanya Berger-Wolf (2025):<br />
<a title="Node attribute prediction in dynamic graphs presents unique challenges due to evolving edge structures and temporal dependencies. Existing spatio-temporal graph neural networks (STGNNs) such as STGCN and DCRNN typically assume static topologies or fixed edge weights, limiting their ability to generalize in settings with dynamic interactions or disconnected nodes. In this work, we introduce \textitDynaSTy, a novel Spatio-Temporal transformer architecture designed for multidimensional node attribute forecasting on DYnamic graphs with temporally evolving edges and node attributes. A variant of DynaSTy incorporates a dynamic graph learner to infer time-specific adjacency matrices directly from node features and applies attention mechanisms across both graph structure and time. Experiments on the METR-LA traffic dataset,and Bitcoin Trust network dataset demonstrate that DynaSTy outperforms classical STGNNs on root mean squared error (RMSE), mean absolute error (MAE), and mean absolute percentage error (MAPE). The model is especially robust to zero-degree nodes and dynamically evolving graph connectivity." href="papers/cameraReady/344/CameraReady/NodeAttributePred_MLG.pdf">A Spatio-Temporal Transformer for Node Attribute Prediction in Dynamic Graphs</a>.<br />
<p class="discreet">
[group A]
[<a href="papers/cameraReady/344/CameraReady/NodeAttributePred_MLG.pdf">pdf</a>]
</p>
</li>
<li>
Pascal Plettenberg, André Alcalde, Bernhard Sick, Josephine Thomas (2025):<br />
<a title="The design and optimization of Printed Circuit Board (PCB) schematics is crucial for the development of high-quality electronic devices. Thereby, an important task is to optimize drafts by adding components that improve the robustness and reliability of the circuit, e.g., pull-up resistors or decoupling capacitors. Since there is a shortage of skilled engineers and manual optimizations are very time-consuming, these best practices are often neglected. However, this typically leads to higher costs for troubleshooting in later development stages as well as shortened product life cycles, resulting in an increased amount of electronic waste that is difficult to recycle. Here, we present an approach for automating the addition of new components into PCB schematics by representing them as bipartite graphs and utilizing a node pair prediction model based on Graph Neural Networks (GNNs). We apply our approach to three highly relevant PCB design optimization tasks and compare the performance of several popular GNN architectures on real-world datasets labeled by human experts. We show that GNNs can solve these problems with high accuracy and demonstrate that our approach offers the potential to automate PCB design optimizations in a time- and cost-efficient manner." href="papers/submission/121/Submission/sub_1079.pdf">Graph Neural Networks for Automatic Addition of Optimizing Components in Printed Circuit Board Schematics</a>.<br />
<p class="discreet">
[group B]
[<a href="papers/submission/121/Submission/sub_1079.pdf">pdf</a>]
</p>
</li>
<li>
Patrick Indri, Tamara Drucks, Thomas Gärtner (2025):<br />
<a title="We propose using homomorphism density vectors to obtain graph embeddings that are both private and expressive. Homomorphism densities are provably highly discriminative and offer a powerful tool for distinguishing non‐isomorphic graphs. By adding noise calibrated to each density's sensitivity, we ensure that the resulting embeddings satisfy formal differential privacy guarantees. Our construction preserves expressivity in expectation, as each private embedding remains unbiased with respect to the true homomorphism densities. We study the trade-off between privacy, utility, and expressivity, both theoretically and empirically, and show that our private embeddings match the accuracy of their non-private counterparts with increased resilience to privacy attacks. " href="papers/cameraReady/270/CameraReady/private_and_expressive_graph_representations.pdf">Private and Expressive Graph Representations</a>.<br />
<p class="discreet">
[group A]
[<a href="papers/cameraReady/270/CameraReady/private_and_expressive_graph_representations.pdf">pdf</a>]
</p>
</li>
<li>
Pavel Prochazka, Michal Mares, Lukas Bajer (2025):<br />
<a title="Graph structure learning aims to discover optimal connectivity patterns that capture semantic relationships in data, with homophily-the tendency for similar nodes to be c    onnected-being a fundamental principle. While contrastive learning has shown remarkable success in representation learning, its connection to homophilic graph structure o    ptimization remains underexplored. We introduce a probabilistic framework that reveals how contrastive learning implicitly learns homophilic adjacency structures through     representation optimization. Our key insight is that learned representations encode pairwise similarities defining task-specific graph structures optimized for homophily.     We establish a theoretical connection between this learned homophilic structure and weighted K-nearest neighbor classification, showing that both approaches can produce     similar clustering structures under well-separated data conditions, though through different optimization paths. This demonstrates one concrete application where contrast    ive learning optimizes graph connectivity for classification tasks. Our work provides initial theoretical foundations connecting contrastive learning and graph structure     learning, suggesting potential applications to adaptive graph construction in graph neural networks. Comprehensive analysis incorporating regularization effects and numer    ical validation across diverse settings remain important directions for future work." href="papers/cameraReady/269/CameraReady/MLG_2025_camera_ready.pdf">Contrastive Learning as Optimal Homophilic Graph Structure Learning</a>.<br />
<p class="discreet">
[group B]
[<a href="papers/cameraReady/269/CameraReady/MLG_2025_camera_ready.pdf">pdf</a>]
</p>
</li>
<li>
Quentin Haenn, Brice Chardin, Mickaël Baron, Allel Hadjali (2025):<br />
<a title="Clustering aims to group data into homogeneous clusters. However, setting parameters such as a cluster count or a dissimilarity bound can be challenging. This paper introduces Curgraph, a novel graph-based and iterative approach for radius-constrained clustering. Curgraph identifies optimal partitions with respect to maximum cluster radius by computing minimum dominating sets across partial graphs. Experimental results demonstrate Curgraph’s effectiveness compared to state-of-the-art algorithms." href="papers/cameraReady/3/CameraReady/iterative_graph_based_radius_constrained_clustering_camera_ready.pdf">Iterative Graph-Based Radius Constraint Clustering</a>.<br />
<p class="discreet">
[group A]
[<a href="papers/cameraReady/3/CameraReady/iterative_graph_based_radius_constrained_clustering_camera_ready.pdf">pdf</a>]
</p>
</li>
<li>
Saku Peltonen, Roger Wattenhofer (2025):<br />
<a title="We analyze the expressive power of Graph Neural Networks for SAT solving through the lens of the Weisfeiler-Leman (WL) test. We prove that certain pairs of 3-SAT instances with O(n) variables are indistinguishable under the n-WL test, despite one being satisfiable and the other unsatisfiable. In contrast, planar SAT — a well-known NP-complete SAT variant — is fully distinguishable by 4-WL. We argue that random SAT instances are largely distinguishable, which we prove for a particular generation method. We validate this hypothesis through experiments on random instances from the G4SAT benchmark." href="papers/cameraReady/206/CameraReady/mlg25-camera.pdf">On the Expressive Power of GNNs for Boolean Satisfiability</a>.<br />
<p class="discreet">
[group B]
[<a href="papers/cameraReady/206/CameraReady/mlg25-camera.pdf">pdf</a>]
</p>
</li>
<li>
Simon Rittel, Sebastian Tschiatschek (2025):<br />
<a title="Bayesian approaches for causal discovery can -in principle- quantify uncertainty in the prediction of the underlying causal structure, typically modeled by a directed acyclic graph (DAG). Various semi-implicit models for parametrized distributions over DAGs have been proposed, but their limitations have not been studied thoroughly. In this work, we focus on the expressiveness of parametrized distributions over DAGs in the context of causal structure learning and show several limitations of candidate models in a theoretical analysis and validate them in experiments. To overcome them, we propose  mixture models of distributions over DAGs." href="papers/cameraReady/272/CameraReady/Dist_DAGs_Causal_Discovery.pdf">Expressivity of Parametrized Distributions over DAGs for Causal Discovery</a>.<br />
<p class="discreet">
[group B]
[<a href="papers/cameraReady/272/CameraReady/Dist_DAGs_Causal_Discovery.pdf">pdf</a>]
</p>
</li>
<li>
Victor Toscano-Duran, Bastian Rieck (2025):<br />
<a title="The shape of a molecule plays a fundamental role in determining its physicochemical and biological properties. However, it is often underrepresented in current molecular representations and learning approaches. In this work, we investigate and propose the use of the Euler Characteristic Transform (ECT) as a topological molecular representation and shape descriptor to capture morphological information from molecules, which are initially represented as graphs. The ECT enables the extraction of multiscale structural features from a geometric and topological perspective, offering a novel way to represent and encode molecular shape.  We assess the predictive performance of this representation on a regression task across nine benchmark datasets, all centered around a specific molecular property as the prediction target: the constant inhibition $K_i$. In addition, we compare our proposed ECT-based representation against traditional molecular representations and methods, as fingerprints, descriptors, and graph neural networks (GNNs). Our results show that our ECT-based representation achieves \emphcompetitive performance, ranking among the best-performing methods on several datasets. More importantly, its combination with traditional representations, particularly with the AVALON fingerprint, significantly \emphenhances prediction performance, outperforming all individual method on most datasets. " href="papers/cameraReady/21/CameraReady/Mining_Learning_with_Graphs_Workshop_ECMLPKDD_CameraReadyVersion.pdf">A Topological Molecular Representation for Molecular Learning Based on the Euler Characteristic Transform</a>.<br />
<p class="discreet">
[group A]
[<a href="papers/cameraReady/21/CameraReady/Mining_Learning_with_Graphs_Workshop_ECMLPKDD_CameraReadyVersion.pdf">pdf</a>]
</p>
</li>
</ol>
 


 <!-- <h2 id="accepted">Invited Main Conference Papers</h2>  -->
 <!-- this will be replaced by autogenerated list -->
 <!-- <ol>
</ol>
 -->


<!-- <h2 id="awards">Awards</h2>

<p>During the workshop we held two community votes. One for the best paper award as well as one for the best poster award. Furthermore, we were able to hand out the AstraZeneca Healthcare & Bio Award. We thank Astra Zeneca for the generous sponsorship of our awards!</p> -->

<!-- <h3> Best Paper</h3> -->
<!-- this will be replaced by autogenerated list -->
<!-- <ol>
</ol>
 -->

<!-- <h3> Best Poster</h3> -->
<!-- this will be replaced by autogenerated list -->
<!-- <ol>
</ol>
 -->

<!-- <h3> AstraZeneca Healthcare & Bio Award</h3> -->
<!-- this will be replaced by autogenerated list -->
<!-- <ol>
</ol>
 -->


<h2 id="contact">Organizers</h2>

<ul class="organizers">
<li class="center">
  <img src="franka.jpg" width="150px" height="auto"><br/>
  <a href="https://dm.cs.univie.ac.at/team/person/112939/" target=_blank>Franka Bause</a><br />
  University of Vienna
</li>
<li class="center">
  <img src="thomas.jpg" width="150px" height="auto"><br/>
  <a href="https://thomasgaertner.org" target=_blank>Thomas Gärtner</a><br />
  TU Wien
</li>
<li class="center">
  <img src="cate.png" width="150px" height="auto"><br/>
  <a href="https://sailab.diism.unisi.it/people/caterina-graziani/" target=_blank>Caterina Graziani</a><br />
  University of Siena
</li>
<li class="center">
  <img src="nils.jpg" width="150px" height="auto"><br/>
  <a href="https://dm.cs.univie.ac.at/team/person/111520/" target=_blank>Nils Kriege</a><br />
  University of Vienna
</li>
<li class="center">
  <img src="yllka_crop.jpeg" width="150px" height="auto"><br/>
  <a href="https://dm.cs.univie.ac.at/team/person/111869/" target=_blank>Yllka Velaj</a><br />
  University of Vienna
</li>
<li class="center">
  <img src="pascal.jpg" width="150px" height="auto"><br/>
  <a href="https://pwelke.de" target=_blank>Pascal Welke</a><br />
  Lancaster University Leipzig<br />TU Wien
</li>
</ul>

<h2 id="pc">Program Committee</h2>
<ul class="twocol">
<li>Alessia Lucia Prete <i>(University of Siena)</i></li>
<li>Alice Moallemy-Oureh <i>(University of Kassel)</i></li>
<li>Anatol Ehrlich <i>(University of Vienna)</i></li>
<li>Andreas Roth <i>(TU Dortmund)</i></li>
<li>Antonio Longa <i>(University of Trento)</i></li>
<li>Celia Rubio-Madrigal <i>(CISPA Helmholtz Center for Information Security)</i></li>
<li>Christopher Blöcker <i>(University of Würzburg)</i></li>
<li>Christoph Sandrock <i>(TU Wien)</i></li>
<li>David B. Blumenthal <i>(FAU)</i></li>
<li>Duccio  Meconcelli <i>(University of Florence)</i></li>
<li>Fabrizio Frasca <i>(Technion)</i></li>
<li>Federica Baccini <i>(Sapienza University of Rome)</i></li>
<li>Filippo Guerranti <i>(Technical University of Munich)</i></li>
<li>Florian Grötschla <i>(ETH Zürich)</i></li>
<li>Florian Seiffarth <i>(University of Bonn)</i></li>
<li>Francesco Giannini <i>(Scuola Normale Superiore)</i></li>
<li>Franco Scarselli <i>(University of Siena)</i></li>
<li>Giuseppe Alessio D'Inverno <i>(SISSA)</i></li>
<li>Ilie Sarpe <i>(KTH Royal Institute of Technology)</i></li>
<li>Ingo Scholtes <i>(University of Würzburg)</i></li>
<li>Johannes Borg Sandberg Petersen <i>(TU Wien)</i></li>
<li>Lorenz Kummer <i>(University of Vienna)</i></li>
<li>Luis Müller <i>(RWTH Aachen)</i> </li>
<li>Manuel Dileo <i>(University of Milan)</i></li>
<li>Marek Dědič <i>(Czech Technical University in Prague)</i></li>
<li>Naheed Anjum Arafat <i>(Howard University)</i></li>
<li>Nicolò Navarin <i>(University of Padova)</i></li>
<li>Katharina Limbeck <i>(Helmholtz Munich, TU Munich)</i></li>
<li>Klaus Weinbauer <i>(TU Wien)</i></li>
<li>Pascal Plettenberg <i>(University of Kassel)</i></li>
<li>Patrick Indri <i>(TU Wien)</i></li>
<li>Pietro Bongini <i>(University of Siena)</i></li>
<li>Sagar Malhotra <i>(TU Wien)</i></li>
<li>Sara Bacconi <i>(University of Siena)</i></li>
<li>Shota Saito <i>(UCL)</i></li>
<li>Sourav Medya <i>(UIC)</i></li>
<li>Stefan Neumann <i>(TU Wien)</i></li>
<li>Tamara Drucks <i>(TU Wien)</i></li>
<li>Till Schulz <i>(Max Planck Institute of Biochemistry)</i></li>
<li>Veronica Lachi <i>(FBK)</i></li>
<li>Vincent Grande <i>(RWTH Aachen University)</i></li>
</ul>


<h2 id="history">Previous Workshops</h2>
<ul class="centered">
  <li><a href=https://mlg-europe.github.io/2024/ target=_blank rel=noopener>2024, Vilnius, Lithuania (co-located with ECMLPKDD)</a></li>
  <li><a href=https://mlg-europe.github.io/2023/ target=_blank rel=noopener>2023, Torino, Italy (co-located with ECMLPKDD)</a></li>
  <li><a href=http://www.mlgworkshop.org/2023/ target=_blank rel=noopener>2023, Long Beach, USA (co-located with KDD)</a></li>
  <li><a href=https://mlg-europe.github.io/2022/ target=_blank rel=noopener>2022, Grenoble, France (co-located with ECMLPKDD)</a></li>
  <li><a href=http://www.mlgworkshop.org/2022/ target=_blank rel=noopener>2022, Washington, USA (co-located with KDD)</a></li>
  <li><a href=http://www.mlgworkshop.org/2020/ target=_blank rel=noopener>2020, virtual (co-located with KDD)</a></li>
  <li><a href=http://www.mlgworkshop.org/2019/ target=_blank rel=noopener>2019, Anchorage, USA (co-located with KDD)</a></li>
  <li><a href=http://www.mlgworkshop.org/2018/ target=_blank rel=noopener>2018, London, United Kingdom (co-located with KDD)</a></li>
  <li><a href=http://www.mlgworkshop.org/2017/ target=_blank rel=noopener>2017, Halifax, Nova Scotia, Canada (co-located with KDD)</a></li>
  <li><a href=http://www.mlgworkshop.org/2016/ target=_blank rel=noopener>2016, San Francisco, USA (co-located with KDD)</a></li>
  <li><a href=http://snap.stanford.edu/mlg2013/ target=_blank rel=noopener>2013, Chicago, USA (co-located with KDD)</a></li>
  <li><a href=http://dtai.cs.kuleuven.be/events/mlg2012/ target=_blank rel=noopener>2012, Edinburgh, Scotland (co-located with ICML)</a></li>
  <li>2011, San Diego, USA (co-located with KDD)</li>
  <li><a href=http://www.cs.umd.edu/mlg2010/ target=_blank rel=noopener>2010, Washington, USA (co-located with KDD)</a></li>
  <li><a href=http://dtai.cs.kuleuven.be/ilp-mlg-srl// target=_blank rel=noopener>2009, Leuven, Belgium (co-located with SRL and ILP)</a></li>
  <li><a href=http://research.ics.aalto.fi/events/MLG08/ target=_blank rel=noopener>2008, Helsinki, Finland (co-located with ICML)</a></li>
  <li>2007, Firenze, Italy</li>
  <li>2006, Berlin, Germany (co-located with ECML and PKDD)</li>
  <li>2005, Porto, Portugal, October 7, 2005 (co-located with ECML and PKDD)</li>
  <li>2004, Pisa, Italy, September 24, 2004 (co-located with ECML and PKDD)</li>
  <li><a href=http://www.ar.sanken.osaka-u.ac.jp/MGTS-2003CFP.html target=_blank rel=noopener>2003, Cavtat-Dubrovnik, Croatia (co-located with ECML and PKDD)</a></li></ul>


<!-- the footer -->
<p class="discreet">This page tries to be minimalistic in layout, bandwith, and used tools. It is hosted on <a href="https://github.com/mlg-europe">github pages</a>, using <a href="https://neat.joeldare.com/">neat.css</a> stylesheets, and <a href="https://bibtexparser.readthedocs.org">bibtexparser</a> to generate the lists of papers.</p>

</body>

</html>
