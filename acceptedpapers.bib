

@article{3},
  publicationtype = {accepted},
  Postersession = {A},
  year = {2025},
  pdf = {papers/cameraReady/3/CameraReady/iterative_graph_based_radius_constrained_clustering_camera_ready.pdf},
  poster = {},
  Paper_ID = {3},
  title = {Iterative Graph-Based Radius Constraint Clustering},
  Abstract = {Clustering aims to group data into homogeneous clusters. However, setting parameters such as a cluster count or a dissimilarity bound can be challenging. This paper introduces Curgraph, a novel graph-based and iterative approach for radius-constrained clustering. Curgraph identifies optimal partitions with respect to maximum cluster radius by computing minimum dominating sets across partial graphs. Experimental results demonstrate Curgraph’s effectiveness compared to state-of-the-art algorithms.},
  author = {Haenn, Quentin and Chardin, Brice and Baron, Mickaël and Hadjali, Allel},
  Status = {Accept},
  Files = {iterative_graph_based_radius_constrained_clustering.pdf (345,826 bytes)},
  Number_of_Files = {1},
  Supplementary_Files = {nan},
  Number_of_Supplementary_Files = {0},
}
@article{21},
  publicationtype = {accepted},
  Postersession = {A},
  year = {2025},
  pdf = {papers/cameraReady/21/CameraReady/Mining_Learning_with_Graphs_Workshop_ECMLPKDD_CameraReadyVersion.pdf},
  poster = {},
  Paper_ID = {21},
  title = {A Topological Molecular Representation for Molecular Learning Based on the Euler Characteristic Transform},
  Abstract = {The shape of a molecule plays a fundamental role in determining its physicochemical and biological properties. However, it is often underrepresented in current molecular representations and learning approaches. In this work, we investigate and propose the use of the Euler Characteristic Transform (ECT) as a topological molecular representation and shape descriptor to capture morphological information from molecules, which are initially represented as graphs. The ECT enables the extraction of multiscale structural features from a geometric and topological perspective, offering a novel way to represent and encode molecular shape.  We assess the predictive performance of this representation on a regression task across nine benchmark datasets, all centered around a specific molecular property as the prediction target: the constant inhibition $K_i$. In addition, we compare our proposed ECT-based representation against traditional molecular representations and methods, as fingerprints, descriptors, and graph neural networks (GNNs). Our results show that our ECT-based representation achieves \emph{competitive performance}, ranking among the best-performing methods on several datasets. More importantly, its combination with traditional representations, particularly with the AVALON fingerprint, significantly \emph{enhances prediction performance}, outperforming all individual method on most datasets. },
  author = {Toscano-Duran, Victor and Rieck, Bastian},
  Status = {Accept},
  Files = {Mining_Learning_with_Graphs_Workshop_ECMLPKDD.pdf (854,477 bytes)},
  Number_of_Files = {1},
  Supplementary_Files = {nan},
  Number_of_Supplementary_Files = {0},
}
@article{66},
  publicationtype = {accepted},
  Postersession = {B},
  year = {2025},
  pdf = {},
  poster = {},
  Paper_ID = {66},
  title = {Oversmoothing, "Oversquashing", Heterophily, Long-Range, and more: Demystifying Common Beliefs in Graph Machine Learning},
  Abstract = {After a renaissance phase in which researchers revisited the message-passing paradigm through the lens of deep learning, the graph machine learning community shifted its attention towards a deeper and practical understanding of message-passing's benefits and limitations. In this position paper, we notice how the fast pace of progress around the topics of oversmoothing and oversquashing, the homophily-heterophily dichotomy, and long-range tasks, came with the consolidation of commonly accepted beliefs and assumptions that are not always true nor easy to distinguish from each other. We argue that this has led to ambiguities around the investigated problems, preventing researchers from focusing on and addressing precise research questions while causing a good amount of misunderstandings. Our contribution wants to make such common beliefs explicit and encourage critical thinking around these topics, supported by simple but noteworthy counterexamples. The hope is to clarify the distinction between the different issues and promote separate but intertwined research directions to address them.},
  author = {Arnaiz-Rodriguez, Adrian and Errica, Federico},
  Status = {Accept},
  Files = {Demystifying_Common_Beliefs_around_OSM_and_OSQ__MLG_Workshop_2025___in_progress_.pdf (898,619 bytes)},
  Number_of_Files = {1},
  Supplementary_Files = {nan},
  Number_of_Supplementary_Files = {0},
}
@article{88},
  publicationtype = {accepted},
  Postersession = {A},
  year = {2025},
  pdf = {papers/submission/88/Submission/MIMO_GCs_MLG.pdf},
  poster = {},
  Paper_ID = {88},
  title = {What Can We Learn From MIMO Graph Convolutions?},
  Abstract = {This work is accepted for publication at IJCAI 2025.  Most graph neural networks (GNNs) utilize approximations of the general graph convolution derived in the graph Fourier domain. While GNNs are typically applied in the multi-input multi-output (MIMO) case, the approximations are performed in the single-input single-output (SISO) case. In this work, we first derive the MIMO graph convolution through the convolution theorem and approximate it directly in the MIMO case. We find the key MIMO-specific property of the graph convolution to be operating on multiple computational graphs, or equivalently, applying distinct feature transformations for each pair of nodes. As a localized approximation, we introduce localized MIMO graph convolutions (LMGCs), which generalize many linear message-passing neural networks. For almost every choice of edge weights, we prove that LMGCs with a single computational graph are injective on multisets, and the resulting representations are linearly independent when more than one computational graph is used. Our experimental results confirm that an LMGC can combine the benefits of various methods.},
  author = {Roth, Andreas and Liebig, Thomas},
  Status = {Accept (Resubmission)},
  Files = {MIMO_GCs_MLG.pdf (552,920 bytes)},
  Number_of_Files = {1},
  Supplementary_Files = {nan},
  Number_of_Supplementary_Files = {0},
}
@article{121},
  publicationtype = {accepted},
  Postersession = {B},
  year = {2025},
  pdf = {papers/submission/121/Submission/sub_1079.pdf},
  poster = {},
  Paper_ID = {121},
  title = {Graph Neural Networks for Automatic Addition of Optimizing Components in Printed Circuit Board Schematics},
  Abstract = {The design and optimization of Printed Circuit Board (PCB) schematics is crucial for the development of high-quality electronic devices. Thereby, an important task is to optimize drafts by adding components that improve the robustness and reliability of the circuit, e.g., pull-up resistors or decoupling capacitors. Since there is a shortage of skilled engineers and manual optimizations are very time-consuming, these best practices are often neglected. However, this typically leads to higher costs for troubleshooting in later development stages as well as shortened product life cycles, resulting in an increased amount of electronic waste that is difficult to recycle. Here, we present an approach for automating the addition of new components into PCB schematics by representing them as bipartite graphs and utilizing a node pair prediction model based on Graph Neural Networks (GNNs). We apply our approach to three highly relevant PCB design optimization tasks and compare the performance of several popular GNN architectures on real-world datasets labeled by human experts. We show that GNNs can solve these problems with high accuracy and demonstrate that our approach offers the potential to automate PCB design optimizations in a time- and cost-efficient manner.},
  author = {Plettenberg, Pascal and Alcalde, André and Sick, Bernhard and Thomas, Josephine},
  Status = {Accept (Resubmission)},
  Files = {sub_1079.pdf (1,564,096 bytes)},
  Number_of_Files = {1},
  Supplementary_Files = {Appendix.pdf (176,616 bytes)},
  Number_of_Supplementary_Files = {1},
}
@article{126},
  publicationtype = {accepted},
  Postersession = {B},
  year = {2025},
  pdf = {papers/submission/126/Submission/4523_Evaluating_explainability-8.pdf},
  poster = {},
  Paper_ID = {126},
  title = {Evaluating explainability techniques on discrete-time graph neural networks},
  Abstract = {Discrete-time temporal Graph Neural Networks (GNNs) are powerful tools for modeling evolving graph-structured data and are widely used in decision-making processes across domains such as social network analysis, financial systems, and collaboration networks. Explaining the predictions of these models is an important research area due to the critical role their decisions play in building trust in social or financial systems. However, the explainability of Temporal Graph Neural Networks remains a challenging and relatively unexplored field. Hence, in this work, we propose a novel framework to evaluate explainability techniques tailored for discrete-time temporal GNNs. Our framework introduces new training and evaluation settings that capture the evolving nature of temporal data, defines metrics to assess the temporal aspects of explanations, and establishes baselines and models specific to discrete-time temporal networks. Through extensive experiments, we outline the best explainability techniques for discrete-time GNNs in terms of fidelity, efficiency, and human-readability trade-offs. By addressing the unique challenges of temporal graph data, our framework sets the stage for future advancements in explaining discrete-time GNNs.},
  author = {Dileo, Manuel and Zignani, Matteo and Gaito, Sabrina},
  Status = {Accept (Resubmission)},
  Files = {4523_Evaluating_explainability-8.pdf (653,078 bytes)},
  Number_of_Files = {1},
  Supplementary_Files = {nan},
  Number_of_Supplementary_Files = {0},
}
@article{141},
  publicationtype = {accepted},
  Postersession = {A},
  year = {2025},
  pdf = {papers/cameraReady/141/CameraReady/LRIM___MLG25-compressed.pdf},
  poster = {},
  Paper_ID = {141},
  title = {Long Range Ising Model: A Benchmark for Long Range Capabilities in Graph Learning},
  Abstract = {Accurately modeling long-range dependencies in graph structured data is critical for many real-world applications. However, properly incorporating long-range interactions beyond the nodes' immediate neighborhood remains an open challenge for graph machine learning models. Existing benchmarks for evaluating long-range capabilities cannot guarantee that their tasks actually depend on long-range information or are artificial. Therefore, claiming long-range modeling improvements based on empirical performance on those datasets remains a fragile and weak form of evidence. We introduce the Long-Range Ising Model (LRIM) Graph Benchmark, a physics-grounded benchmark based on the well-studied Ising model whose ground truth theoretically depends on long-range dependencies. Our benchmark consists of multiple datasets that scale from 256 to 65k nodes per graph and provide controllable long-range dependencies through multiple tunable parameters, allowing precise control over the hardness and "long-rangedness" of tasks. We provide model-agnostic evidence showing that local information is insufficient, further validating the design choices of our benchmark. This is ongoing, new research to provide a framework towards principled and provable long-range capability evaluation for graph machine learning.},
  author = {Mathys, Joël and Christiansen, Henrik and Errica, Federico and Alesiani, Francesco},
  Status = {Accept},
  Files = {LRIM___MLG25-6.pdf (3,748,648 bytes)},
  Number_of_Files = {1},
  Supplementary_Files = {nan},
  Number_of_Supplementary_Files = {0},
}
@article{146},
  publicationtype = {accepted},
  Postersession = {A},
  year = {2025},
  pdf = {papers/cameraReady/146/CameraReady/_MLG2025__MagEdgePool.pdf},
  poster = {},
  Paper_ID = {146},
  title = {Geometry-Aware Edge Pooling for Graph Neural Networks},
  Abstract = {Abstract. Graph Neural Networks (GNNs) have shown significant success for graph-based tasks. Pooling layers are crucial components of GNNs that enable faster training and potentially better generalisation by reducing the size of input graphs. However, existing pooling operations often optimise for the learning task at the expense of fundamental graph structures and interpretability. This leads to unreliable performance across varying dataset types, downstream tasks and pooling ratios. Addressing these concerns, we propose novel graph pooling layers for structure-aware pooling via edge collapses. Our methods leverage diffusion geometry and iteratively reduce a graph’s size while preserving both its metric structure and structural diversity. We guide pooling using magnitude, an isometry-invariant diversity measure, which permits us to control the fidelity of the pooling process. Further, we use the spread of a metric space as a faster and more stable alternative ensuring computational efficiency. Empirical results demonstrate that our methods (i) achieve superior performance compared to alternative pooling layers across a range of diverse graph classification tasks, (ii) preserve key spectral properties of the input graphs, and (iii) retain high accuracy across varying pooling ratios.},
  author = {Limbeck, Katharina and Mezrag, Lydia and Wolf, Guy and Rieck, Bastian},
  Status = {Accept},
  Files = {MLG2025.pdf (1,767,379 bytes)},
  Number_of_Files = {1},
  Supplementary_Files = {MLG2025_appendix.pdf (1,767,397 bytes)},
  Number_of_Supplementary_Files = {1},
}
@article{188},
  publicationtype = {accepted},
  Postersession = {A},
  year = {2025},
  pdf = {papers/submission/188/Submission/Spectral_Pruning_MLG25_2.pdf},
  poster = {},
  Paper_ID = {188},
  title = {Spectral Graph Pruning Against Over-Squashing and Over-Smoothing},
  Abstract = {Message Passing Graph Neural Networks are known to suffer from two problems that are sometimes believed to be diametrically opposed: over-squashing and over-smoothing. The former results from topological bottlenecks that hamper the information flow from distant nodes and are mitigated by spectral gap maximization, primarily, by means of edge additions. However, such additions often promote over-smoothing that renders nodes of different classes less distinguishable. Inspired by the Braess phenomenon, we argue that deleting edges can address over-squashing and over-smoothing simultaneously. This insight explains how edge deletions can improve generalization, thus connecting spectral gap optimization to a seemingly disconnected objective of reducing computational resources by pruning graphs for lottery tickets. To this end, we propose a computationally effective spectral gap optimization framework to add or delete edges and demonstrate its effectiveness on the long range graph benchmark and on larger heterophilous datasets.},
  author = {Jamadandi, Adarsh and Rubio-Madrigal, Celia and Burkholz, Rebekka},
  Status = {Accept (Resubmission)},
  Files = {Spectral_Pruning_MLG25 (2).pdf (3,758,979 bytes)},
  Number_of_Files = {1},
  Supplementary_Files = {nan},
  Number_of_Supplementary_Files = {0},
}
@article{198},
  publicationtype = {accepted},
  Postersession = {B},
  year = {2025},
  pdf = {papers/submission/198/Submission/ComFy_MLG25.pdf},
  poster = {},
  Paper_ID = {198},
  title = {GNNs Getting ComFy: Community and Feature Similarity Guided Rewiring},
  Abstract = {Maximizing the spectral gap through graph rewiring has been proposed to enhance the performance of message-passing graph neural networks (GNNs) by addressing over-squashing. However, as we show, minimizing the spectral gap can also improve generalization. To explain this, we analyze how rewiring can benefit GNNs within the context of stochastic block models. Since spectral gap optimization primarily influences community strength, it improves performance when the community structure aligns with node labels. Building on this insight, we propose three distinct rewiring strategies that explicitly target community structure, node labels, and their alignment: (a) community structure-based rewiring (ComMa), a more computationally efficient alternative to spectral gap optimization that achieves similar goals and (b) feature similarity-based rewiring (FeaSt), which focuses on maximizing global homophily and and (c) a hybrid approach (ComFy), which enhances local feature similarity while preserving community structure to optimize label-community alignment. Extensive experiments confirm the effectiveness of these strategies and support our theoretical insights.},
  author = {Rubio-Madrigal, Celia and Jamadandi, Adarsh and Burkholz, Rebekka},
  Status = {Accept (Resubmission)},
  Files = {ComFy_MLG25.pdf (1,914,088 bytes)},
  Number_of_Files = {1},
  Supplementary_Files = {nan},
  Number_of_Supplementary_Files = {0},
}
@article{202},
  publicationtype = {accepted},
  Postersession = {B},
  year = {2025},
  pdf = {papers/submission/202/Submission/paper.pdf},
  poster = {},
  Paper_ID = {202},
  title = {Efficient Approximate Temporal Triangle Counting in Streaming with Predictions},
  Abstract = {[Paper accepted at the ECML-PKDD 2025 research track]  Triangle counting is a fundamental and widely studied problem on static graphs, and recently on temporal graphs, where edges carry information on the timings of the associated events. Streaming processing and resource efficiency are crucial requirements for counting triangles in modern massive temporal graphs, with millions of nodes and up to billions of temporal edges. However, current exact and approximate algorithms are unable to handle large-scale temporal graphs.  To fill such a gap, we introduce STEP, a scalable and efficient algorithm to approximate temporal triangle counts from a stream of temporal edges.  STEP combines predictions of the number of triangles a temporal edge is involved in, with a simple sampling strategy, leading to scalability, efficiency, and accurate approximation of all eight temporal triangle types simultaneously. We analytically prove that, by using a sublinear amount of memory, STEP obtains unbiased and very accurate estimates.  In fact, even noisy predictions can significantly reduce the variance of STEP's estimates.  Our extensive experiments on massive temporal graphs with up to billions of edges demonstrate that STEP outputs high-quality estimates and is more efficient than state-of-the-art methods.},
  author = {Venturin, Giorgio and Sarpe, Ilie and Vandin, Fabio},
  Status = {Accept (Resubmission)},
  Files = {paper.pdf (2,010,352 bytes)},
  Number_of_Files = {1},
  Supplementary_Files = {appendix.pdf (2,929,999 bytes)},
  Number_of_Supplementary_Files = {1},
}
@article{206},
  publicationtype = {accepted},
  Postersession = {B},
  year = {2025},
  pdf = {papers/cameraReady/206/CameraReady/mlg25-camera.pdf},
  poster = {},
  Paper_ID = {206},
  title = {On the Expressive Power of GNNs for Boolean Satisfiability},
  Abstract = {We analyze the expressive power of Graph Neural Networks for SAT solving through the lens of the Weisfeiler-Leman (WL) test. We prove that certain pairs of 3-SAT instances with O(n) variables are indistinguishable under the n-WL test, despite one being satisfiable and the other unsatisfiable. In contrast, planar SAT — a well-known NP-complete SAT variant — is fully distinguishable by 4-WL. We argue that random SAT instances are largely distinguishable, which we prove for a particular generation method. We validate this hypothesis through experiments on random instances from the G4SAT benchmark.},
  author = {Peltonen, Saku and Wattenhofer, Roger},
  Status = {Accept},
  Files = {ecml-mlg25.pdf (593,187 bytes)},
  Number_of_Files = {1},
  Supplementary_Files = {nan},
  Number_of_Supplementary_Files = {0},
}
@article{257},
  publicationtype = {accepted},
  Postersession = {A},
  year = {2025},
  pdf = {papers/cameraReady/257/CameraReady/MLG2025-13.pdf},
  poster = {},
  Paper_ID = {257},
  title = {Graph Product Representations},
  Abstract = {We propose a novel family of algorithms for generating expressive graph and node representations utilizing graph products. These representations, based on simple substructure counts in product graphs, aim to encode structural information that is often missed by standard message passing graph neural networks (MPNNs).  MPNNs allow to learn vector representations of graphs, that are used in critical domains like drug discovery, social network analysis, protein folding and transportation networks. Their expressiveness is limited by the Weisfeiler-Leman (WL) graph isomorphism test, meaning they are unable to distinguish certain non-isomorphic graphs and fail to recognize important substructures, which restricts their overall capability. Our approach, called  Product Substructure Count (PSC), addresses this by utilizing graph products to transform graphs. The transformed graphs encode extensive structural information within simple substructures, such as cycles. By counting these substructures at both the graph and node levels, we can generate embeddings that represent the graph as a whole as well as individual nodes.    We show that PSC representations outperform WL in isomorphism testing and improve the representational capacity of MPNNs across multiple benchmark datasets.},
  author = {Seeliger, Maximilian and Jogl, Fabian and Gärtner, Thomas},
  Status = {Accept},
  Files = {MLG2025-final-submission.pdf (4,578,915 bytes)},
  Number_of_Files = {1},
  Supplementary_Files = {nan},
  Number_of_Supplementary_Files = {0},
}
@article{269},
  publicationtype = {accepted},
  Postersession = {B},
  year = {2025},
  pdf = {papers/cameraReady/269/CameraReady/MLG_2025_camera_ready.pdf},
  poster = {},
  Paper_ID = {269},
  title = {Contrastive Learning as Optimal Homophilic Graph Structure Learning},
  Abstract = {Graph structure learning aims to discover optimal connectivity patterns that capture semantic relationships in data, with homophily-the tendency for similar nodes to be c    onnected-being a fundamental principle. While contrastive learning has shown remarkable success in representation learning, its connection to homophilic graph structure o    ptimization remains underexplored. We introduce a probabilistic framework that reveals how contrastive learning implicitly learns homophilic adjacency structures through     representation optimization. Our key insight is that learned representations encode pairwise similarities defining task-specific graph structures optimized for homophily.     We establish a theoretical connection between this learned homophilic structure and weighted K-nearest neighbor classification, showing that both approaches can produce     similar clustering structures under well-separated data conditions, though through different optimization paths. This demonstrates one concrete application where contrast    ive learning optimizes graph connectivity for classification tasks. Our work provides initial theoretical foundations connecting contrastive learning and graph structure     learning, suggesting potential applications to adaptive graph construction in graph neural networks. Comprehensive analysis incorporating regularization effects and numer    ical validation across diverse settings remain important directions for future work.},
  author = {Prochazka, Pavel and Mares, Michal and Bajer, Lukas},
  Status = {Accept},
  Files = {paper.pdf (354,872 bytes)},
  Number_of_Files = {1},
  Supplementary_Files = {nan},
  Number_of_Supplementary_Files = {0},
}
@article{270},
  publicationtype = {accepted},
  Postersession = {A},
  year = {2025},
  pdf = {papers/submission/270/Submission/private_and_expressive_graph_representations.pdf},
  poster = {},
  Paper_ID = {270},
  title = {Private and Expressive Graph Representations},
  Abstract = {We propose using homomorphism density vectors to obtain graph embeddings that are both private and expressive. Homomorphism densities are provably highly discriminative and offer a powerful tool for distinguishing non‐isomorphic graphs. By adding noise calibrated to each density's sensitivity, we ensure that the resulting embeddings satisfy formal differential privacy guarantees. Our construction preserves expressivity in expectation, as each private embedding remains unbiased with respect to the true homomorphism densities. We study the trade-off between privacy, utility, and expressivity, both theoretically and empirically, and show that our private embeddings match the accuracy of their non-private counterparts with increased resilience to privacy attacks. },
  author = {Indri, Patrick and Drucks, Tamara and Gärtner, Thomas},
  Status = {Accept},
  Files = {private_and_expressive_graph_representations.pdf (717,586 bytes)},
  Number_of_Files = {1},
  Supplementary_Files = {nan},
  Number_of_Supplementary_Files = {0},
}
@article{272},
  publicationtype = {accepted},
  Postersession = {B},
  year = {2025},
  pdf = {papers/cameraReady/272/CameraReady/Dist_DAGs_Causal_Discovery.pdf},
  poster = {},
  Paper_ID = {272},
  title = {Expressivity of Parametrized Distributions over DAGs for Causal Discovery},
  Abstract = {Bayesian approaches for causal discovery can -in principle- quantify uncertainty in the prediction of the underlying causal structure, typically modeled by a directed acyclic graph (DAG). Various semi-implicit models for parametrized distributions over DAGs have been proposed, but their limitations have not been studied thoroughly. In this work, we focus on the expressiveness of parametrized distributions over DAGs in the context of causal structure learning and show several limitations of candidate models in a theoretical analysis and validate them in experiments. To overcome them, we propose  mixture models of distributions over DAGs.},
  author = {Rittel, Simon and Tschiatschek, Sebastian},
  Status = {Accept},
  Files = {Expressivity_DAG_Distributions.pdf (643,150 bytes)},
  Number_of_Files = {1},
  Supplementary_Files = {nan},
  Number_of_Supplementary_Files = {0},
}
@article{274},
  publicationtype = {accepted},
  Postersession = {B},
  year = {2025},
  pdf = {papers/cameraReady/274/CameraReady/HGCN_MLG2025_final.pdf},
  poster = {},
  Paper_ID = {274},
  title = {Do We Need Curved Spaces? A Critical Look at Hyperbolic Graph Learning in Graph Classification},
  Abstract = {Hyperbolic geometry has gained attention for its ability to naturally embed hierarchical and tree-like structures with low distortion, outperforming Euclidean spaces in various graph representation tasks. While previous work has demonstrated the advantage of graph neural networks embedding in hyperbolic space for link prediction and node classification, the benefits for graph classification, and especially the role of node features, remain less understood. These studies typically attribute the benefits of hyperbolic models to the hierarchical or tree-like nature of graph structures, often neglecting the important role that node features play in leveraging these geometric advantages. With this in mind, we design an experiment specifically aimed at evaluating the interplay between geometry and node features in graph classification, creating a dataset composed exclusively of tree-structured graphs. Each graph is generated by sampling the number of children per node at each level from a predefined range of branching factors, which varies across levels. The dataset defines two distinct classes based on these branching factor patterns. Nodes are characterized by either random features or structural embeddings obtained via node2vec using breadth-first search (BFS)-biased random walks, designed to emphasize local neighborhood structure and capture hierarchical structures within the graph. We evaluated and compared graph neural networks with node embeddings learned in different geometries— hyperbolic vs. Euclidean— in low-dimensional latent spaces. Results show that hyperbolic geometry outperforms Euclidean models, with the largest gains observed when node features encode local hierarchical context. These findings underscore that the performance advantage of hyperbolic models does not stem solely from the tree-like structure of the graph and rather, effective feature design that captures the underlying hierarchy is essential to fully exploit the benefits of hyperbolic geometry.},
  author = {Naddeo, Dionisia and Azevedo, Tiago and Toschi, Nicola},
  Status = {Accept},
  Files = {DoWeNeedCurvedSpaces_MLG2025.pdf (474,570 bytes)},
  Number_of_Files = {1},
  Supplementary_Files = {nan},
  Number_of_Supplementary_Files = {0},
}
@article{277},
  publicationtype = {accepted},
  Postersession = {B},
  year = {2025},
  pdf = {papers/cameraReady/277/CameraReady/efficient_minimitazion_of_peakless_functions_on_graphs.pdf},
  poster = {},
  Paper_ID = {277},
  title = {Efficient Minimization of Peakless Functions on Bounded-degree Graphs},
  Abstract = {We study the problem of query-efficiently minimizing peakless functions on graphs. Peakless functions are a notion of convex functions on the nodes of a graph and are defined by having no local ``peaks'' along any shortest paths. We demonstrate that peakless functions form a rich class and are non-trivial to optimize on chordal graphs. Further, we show that these functions can be minimized using at most $4 \Delta \log n$ queries on graphs with $n$ nodes and maximum degree $\Delta$, where each query reveals the function value of a selected node. We complement this result by providing a nearly matching lower bound on the query complexity of $\Omega(\frac{\Delta}{\log \Delta}\log n)$.},
  author = {Sandrock, Christoph and Lüderssen, Sebastian and Thiessen, Maximilian and Gärtner, Thomas},
  Status = {Accept},
  Files = {efficient_minimitazion_of_peakless_functions_on_graphs.pdf (369,582 bytes)},
  Number_of_Files = {1},
  Supplementary_Files = {nan},
  Number_of_Supplementary_Files = {0},
}
@article{280},
  publicationtype = {accepted},
  Postersession = {A},
  year = {2025},
  pdf = {papers/cameraReady/280/CameraReady/Graph_AD_Images_CR.pdf},
  poster = {},
  Paper_ID = {280},
  title = {From Pixels to Graphs: Deep Graph-Level Anomaly Detection on Dermoscopic Images},
  Abstract = {Graph Neural Networks (GNNs) have emerged as a powerful approach for graph-based machine learning tasks. Previous work applied GNNs to image-derived graph representations for various downstream tasks such as classification or anomaly detection. These transformations include segmenting images, extracting features from segments, mapping them to nodes, and connecting them. However, to the best of our knowledge, no study has rigorously compared the effectiveness of the numerous potential image-to-graph transformation approaches for GNN-based graph-level anomaly detection (GLAD). In this study, we systematically evaluate the efficacy of multiple segmentation schemes, edge construction strategies, and node feature sets based on color, texture, and shape descriptors to produce suitable image-derived graph representations to perform graph-level anomaly detection. We conduct extensive experiments on dermoscopic images using state-of-the-art GLAD models, examining performance and efficiency in purely unsupervised, weakly supervised, and fully supervised regimes. Our findings reveal, for example, that color descriptors contribute the best standalone performance while incorporating shape and texture features consistently enhances detection efficacy. In particular, our best unsupervised configuration using OCGTL achieves a competitive AUC-ROC score of up to 0.805 without relying on pretrained backbones like comparable image-based approaches. With the inclusion of sparse labels, the performance increases substantially to 0.872 and with full supervision to 0.914 AUC-ROC.},
  author = {Xu, Dehn and Katzke, Tim and Müller, Emmanuel},
  Status = {Accept},
  Files = {Graph_AD_Images-13.pdf (12,936,747 bytes)},
  Number_of_Files = {1},
  Supplementary_Files = {nan},
  Number_of_Supplementary_Files = {0},
}
@article{295},
  publicationtype = {accepted},
  Postersession = {A},
  year = {2025},
  pdf = {papers/submission/295/Submission/2025_ECML_UTMPNN.pdf},
  poster = {},
  Paper_ID = {295},
  title = {Link Prediction with Untrained Message Passing Layers},
  Abstract = {In this work, we explore the use of untrained message passing layers in graph neural networks for link prediction. The untrained message passing layers we consider are derived from widely used graph neural network architectures by removing trainable parameters and nonlinearities in their respective message passing layers. Experimentally, we find that untrained message passing layers can lead to competitive and even superior link prediction performance compared to fully trained message passing layers while being more efficient and naturally interpretable, especially in the presence of high-dimensional features. We also provide a theoretical analysis of untrained message passing layers in the context of link prediction and show that the inner product of features produced by untrained message passing layers relate to common neighbour and path-based topological measures which are widely used for link prediction. As such, untrained message passing layers offer a more efficient and interpretable alternative to trained message passing layers in link prediction tasks.},
  author = {Qarkaxhija, Lisi and Wegner, Anatol and Scholtes, Ingo},
  Status = {Accept},
  Files = {2025_ECML_UTMPNN.pdf (601,651 bytes)},
  Number_of_Files = {1},
  Supplementary_Files = {nan},
  Number_of_Supplementary_Files = {0},
}
@article{335},
  publicationtype = {accepted},
  Postersession = {B},
  year = {2025},
  pdf = {papers/cameraReady/335/CameraReady/Task-Agnostic_Contrastive_Pretraining_for_Relational_Deep_Learning_CR.pdf},
  poster = {},
  Paper_ID = {335},
  title = {Task-Agnostic Contrastive Pretraining for Relational Deep Learning},
  Abstract = {Relational Deep Learning (RDL) is an emerging paradigm that leverages Graph Neural Network principles to learn directly from relational databases by representing them as heterogeneous graphs. However, existing RDL models typically rely on task-specific supervised learning, requiring training separate models for each predictive task, which may hamper scalability and reuse.  In this work, we propose a novel task-agnostic contrastive pretraining approach for RDL that enables database-wide representation learning. For that aim, we introduce three levels of contrastive objectives—row-level, link-level, and context-level—designed to capture the structural and semantic heterogeneity inherent to relational data. We implement the respective pretraining approach through a modular RDL architecture and an efficient sampling strategy tailored to the heterogeneous database setting. Our preliminary results on standard RDL benchmarks demonstrate that fine-tuning the pretrained models measurably outperforms training from scratch, validating the promise of the proposed methodology in learning transferable representations for relational data.},
  author = {Peleška, Jakub and Šír, Gustav},
  Status = {Accept},
  Files = {Task-Agnostic_Contrastive_Pretraining_for_Relational_Deep_Learning.pdf (418,916 bytes)},
  Number_of_Files = {1},
  Supplementary_Files = {nan},
  Number_of_Supplementary_Files = {0},
}
@article{336},
  publicationtype = {accepted},
  Postersession = {B},
  year = {2025},
  pdf = {papers/cameraReady/336/CameraReady/MLG20251.pdf},
  poster = {},
  Paper_ID = {336},
  title = {Late and Early Fusion Graph Neural Network Architectures for Integrative Modeling of Multimodal Brain Connectivity Graphs},
  Abstract = {The integration of structural and functional brain connectivity provides a holistic view of the brain's organization, but its application in Graph Neural Network (GNN) models for predicting "brain age" is understudied, and a systematic benchmark of optimal data fusion strategies is currently lacking. We systematically benchmark the performances of early and late fusion  multimodal architectures  against single-modality models for brain age preddiction using structural and functional connectcomes, using five different GNN backbones on 747 healty partecipants  (median age, 16.3 years and IQR 13.5-18.5 years) obtained from the Philadelphia Neurodevelopmental Cohort. The late fusion architecture improved performance over the structural-only baseline in three of five models, with the GCN model achieving the highest overall score in cross-validation ( R^2=0.639 +- 0.05). The early fusion architecture showed inconsistent results and did not offer a reliable improvement over the single-modality baseline. Finally is observed that optimal model architecture depends on the data type: structural brain graphs favors deep, narrow models to capture their hierarchy, whereas functional brain graphs requires wider, shallower models.},
  author = {Comparini, Alessio and Schmidt, Lea and Siffredi, Vanessa  and Marie, Damien and James, Clara  and Richiardi, Jonas},
  Status = {Accept},
  Files = {Late and Early Fusion Graph Neural Network Architectures for Integrative Modeling of Multimodal Brain Connectivity Graphs.pdf (960,743 bytes)},
  Number_of_Files = {1},
  Supplementary_Files = {nan},
  Number_of_Supplementary_Files = {0},
}
@article{344},
  publicationtype = {accepted},
  Postersession = {A},
  year = {2025},
  pdf = {papers/cameraReady/344/CameraReady/NodeAttributePred_MLG.pdf},
  poster = {},
  Paper_ID = {344},
  title = {A Spatio-Temporal Transformer for Node Attribute Prediction in Dynamic Graphs},
  Abstract = {Node attribute prediction in dynamic graphs presents unique challenges due to evolving edge structures and temporal dependencies. Existing spatio-temporal graph neural networks (STGNNs) such as STGCN and DCRNN typically assume static topologies or fixed edge weights, limiting their ability to generalize in settings with dynamic interactions or disconnected nodes. In this work, we introduce \textit{DynaSTy}, a novel Spatio-Temporal transformer architecture designed for multidimensional node attribute forecasting on DYnamic graphs with temporally evolving edges and node attributes. A variant of DynaSTy incorporates a dynamic graph learner to infer time-specific adjacency matrices directly from node features and applies attention mechanisms across both graph structure and time. Experiments on the METR-LA traffic dataset,and Bitcoin Trust network dataset demonstrate that DynaSTy outperforms classical STGNNs on root mean squared error (RMSE), mean absolute error (MAE), and mean absolute percentage error (MAPE). The model is especially robust to zero-degree nodes and dynamically evolving graph connectivity.},
  author = {Banerji, Namrata and Berger-Wolf, Tanya},
  Status = {Accept},
  Files = {Dynasty.pdf (380,672 bytes)},
  Number_of_Files = {1},
  Supplementary_Files = {nan},
  Number_of_Supplementary_Files = {0},
}
